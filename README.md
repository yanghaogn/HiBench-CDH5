
# HiBench-CDH5 #
## HiBench - the Hadoop Benchmark Suite for CDH5 && Hadoop2 ##

---
- Release version: 1.0
- Release date: 2014/6/29
- Contact: [Yang Hao](mailto:yanghaogn@163.com.com) 
- Homepage: https://github.com/yanghaogn/HiBench-CDH5

- Contents:
    1. Overview
    2. Getting Started
    3. Running

---
### OVERVIEW ###
The [HiBench](https://github.com/intel-hadoop/Hibench) is developped by Intel for Hadoop BenchMark. Since the Hadoop2 is very different from Hadoop1, and the  [HiBench](https://github.com/intel-hadoop/Hibench) is not available for Hadoop1, so I change it and it can support Hadoop2.


  **Micro Benchmarks:**

1. Sort (sort)

    This workload sorts its *text* input data, which is generated using the Hadoop RandomTextWriter example.

2. WordCount (wordcount)

    This workload counts the occurrence of each word in the input data, which are generated using the Hadoop RandomTextWriter example. It is representative of another typical class of real world MapReduce jobs - extracting a small amount of interesting data from large data set.

3. TeraSort (terasort)

    TeraSort is a standard benchmark created by Jim Gray. Its input data is generated by Hadoop TeraGen example program.

  **HDFS Benchmarks:**

4. enhanced DFSIO (dfsioe)

    Enhanced DFSIO tests the HDFS throughput of the Hadoop cluster by generating a large number of tasks performing writes and reads simultaneously. It measures the average I/O rate of each map task, the average throughput of each map task, and the aggregated throughput of HDFS cluster.

  **Machine Learning Benchmarks:**

5. Mahout K-means clustering (kmeans)
    
    This workload tests the K-means (a well-known clustering algorithm for knowledge discovery and data mining) clustering in Mahout 0.7. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution.


---
### Getting Started ###

2. Prerequisites 

  1. Setup HiBench-CDH5

      Download/checkout HiBench-CDH5 benchmark suite from [https://codeload.github.com/yanghaogn/HiBench-CDH5/zip/master](https://codeload.github.com/yanghaogn/HiBench-CDH5/zip/master)


  2. Setup Hadoop

      Before you run any workload in the package, please verify the Hadoop framework is running correctly. All the workloads have been tested with Cloudera Distribution of Hadoop(cdh5)

  3. Setup Mahout (for machine learning benchmarks)
    
      Please make sure you have properly set up Mahout in your cluster if you want to test machine learning benchmarks.

2. Configure for the all workloads

    You need to set some global environment variables in the `bin/hibench-config.sh` file located in the root dir.

          HADOOP_HOME      <The Hadoop installation location>
          HADOOP_CONF_DIR  <The hadoop configuration DIR, default is $HADOOP_HOME/conf>
          COMPRESS_GLOBAL  <Whether to enable the in/out compression for all workloads, 0 is disable, 1 is enable>
          COMPRESS_CODEC_GLOBAL  <The default codec used for in/out data compression>

    Note: Do not change the default values of other global environment variables unless necessary.

3. Configure each workload

    You can modify the `conf/configure.sh` file under each workload folder if it exists. All the data size and options related to the workload are defined in this file. 

4. Synchronize the time on all nodes (This is required for dfsioe, and optional for others)

---
### Running ###

- Run several workloads together

  The `conf/benchmarks.lst` file under the package folder defines the workloads to run when you execute the `bin/run-all.sh` script under the package folder. Each line in the list file specifies one workload. You can use `#` at the beginning of each line to skip the corresponding bench if necessary. 

- Run each workload separately

  You can also run each workload separately. In general, there are 3 different files under one workload folder.

        conf/configure.sh	Configuration file contains all parameters such as data size and test options.
        bin/prepare*.sh   Generate or copy the job input data into HDFS.
        bin/run*.sh       Execute the workload

  Follow the steps below to run a workload

  1. Configure the benchmark: 
      
      set your own configurations by modifying `configure.sh` if necessary
  2. Prepare data: 
      
      `bin/prepare.sh` (`bin/prepare-read.sh` for dfsioe) to prepare input data in HDFS for running the benchmark
  3. Run the benchmark:
      
      `bin/run*.sh` to run the corresponding benchmark
